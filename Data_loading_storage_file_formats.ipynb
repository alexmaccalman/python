{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize= (10,6))\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex1.csv'\n",
    "df = pd.read_csv(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex1.csv'\n",
    "df = pd.read_csv(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex2.csv' # this file does not have a header row\n",
    "df = pd.read_csv(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, header=None) # use this when there is no header\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, names=['a', 'b', 'c', 'd', 'message']) # use this when there is no header row\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['a', 'b', 'c', 'd', 'message'] # defines the columns names\n",
    "df = pd.read_csv(url, names=names, index_col='message') # sets the message column to be the index labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/csv_mindex.csv'\n",
    "\n",
    "df = pd.read_csv(url, index_col=['key1', 'key2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this table doesn't have a fixed delimiter, saved as a txt file.\n",
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex3.txt'\n",
    "\n",
    "# the sep is a regex for any one or more white space characters \\s+\n",
    "df = pd.read_csv(url, sep='\\s+') # pandas infers that the first column is the index since it has no header\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex4.csv'\n",
    "\n",
    "df = pd.read_csv(url, skiprows=[0,2,3]) # this skips the first, third, and fouth rows\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex5.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "pd.isnull(df) # returns a boolean matrix indicating the null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can specify sentinels we want for missing vlaues for each column in a dic\n",
    "sentinels = {'message': ['foo', 'NA'], 'something': ['two']} # sets the values we want assigned as NaN\n",
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex5.csv'\n",
    "\n",
    "df = pd.read_csv(url, na_values=sentinels)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex6.csv'\n",
    "\n",
    "df = pd.read_csv(url, nrows=5) # only loads the first 5 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split into chunck\n",
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex6.csv'\n",
    "\n",
    "chuncker = pd.read_csv(url, chunksize=1000) # only loads the first 5 rows\n",
    "chuncker # returns a TextFileReader object we can iterate over\n",
    "tot = pd.Series()\n",
    "for piece in chuncker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "tot= tot.sort_values(ascending=False)\n",
    "tot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Text Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex5.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# this prints to the console with the '|' as the specified delimiter\n",
    "data.to_csv(sys.stdout, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing vlaues appear as empty strings or spaces. We can use a sentinal value to fix this\n",
    "data.to_csv(sys.stdout, na_rep='NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be default, row and colimn labels are assigned. We can disable this\n",
    "data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delimited Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex7.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data.to_csv('ex7.csv') # this write a .csv file to the current directory\n",
    "import csv # this library helps us when there is malfrmed lines in a .csv\n",
    "f = open('ex7.csv') # creates a TextIOWrapper\n",
    "reader = csv.reader(f) # f is ther iterable object\n",
    "# Iterating through the reader like a file yields tuples of values with any quote characters removed\n",
    "for line in reader:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to wrangle this into the form we need we first read the files into a list of lines\n",
    "with open('ex7.csv') as f:\n",
    "    lines = list(csv.reader(f))\n",
    "# then we split the lines into the header line and the data lines\n",
    "header, values = lines[0], lines[1:] # the lines[1:] means we start at the second index and assign the rest to the values\n",
    "# next we create a dictionary of data columsn to transpose rows to columns\n",
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to define a new format with a different delimiter, string quoting, or line terminator, we define a simple subclass of csv.Dialect\n",
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "# here we use the subclass to apply the new format\n",
    "with open('ex7.csv') as f:\n",
    "    reader = csv.reader(f, dialect=my_dialect)\n",
    "    for line in reader:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we use a '|' delimeter\n",
    "with open('ex7.csv') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for line in reader:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('ex7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Data\n",
    "the object types are dictionaries, arrays( list), strings, numbers, booleand, and nulls. All keys in an object must be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example JSON\n",
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38,\n",
    "               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\"\n",
    "import json\n",
    "result = json.loads(obj) # coverts a JSON string to Python form.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result) # converts a Python object back into JSON\n",
    "asjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass a list of dicts (which were previousy JSON objects) to the DataFrame constructor and select a subset of the data fields\n",
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/example.json'\n",
    "\n",
    "data = pd.read_json(url)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.to_json()) # exports data from pandas to JSON\n",
    "print(data.to_json(orient='records')) # creates a different orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML and HTML: Web Scraping\n",
    "conda install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    \n",
    "    data = pd.read_json(url)\n",
    "    return data\n",
    "\n",
    "download_file(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/fdic_failed_bank_list.html'\n",
    "tables = pd.read_html(url)\n",
    "print(tables)\n",
    "print(type(tables))\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tables[0] # pulls out list\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of bank failures by year\n",
    "close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "print(close_timestamps.head())\n",
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing XML with lxml.objectify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download a file from the web, use requests.get and write to the file format of chioce\n",
    "url = 'https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/mta_perf/Performance_MNR.xml'\n",
    "r = requests.get(url)\n",
    "with open(\"MNR.xml\", 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lxml.objectify, we parse the file and get a referecne to the root node of the xml file with getroot\n",
    "from lxml import objectify\n",
    "path ='MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()\n",
    "\n",
    "os.remove('MNR.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root.INDICATOR returns a generator yielding each <INDICATOR> XML element. For each record, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding a few tags)\n",
    "print(root.INDICATOR)\n",
    "\n",
    "data = []\n",
    "\n",
    "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
    "               'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame(data) # convert this list of dicts into a DataFrame\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML can get much more complicated. Each tag can have metadata\n",
    "from io import StringIO\n",
    "tag = '<a href=\"http://www.google.com\">Google</a>'\n",
    "root = objectify.parse(StringIO(tag)).getroot()\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Data Formats\n",
    "best way to store data in a binary format is with Python's pickle serialization. Picke are recommended for a short term storage format. They are hard to keep stable over time with new format versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/ex1.csv'\n",
    "frame = pd.read_csv(url)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_pickle('frame_pickle') # saves to a pickle file\n",
    "pd.read_pickle('frame_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('frame_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Excel Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the excel file from web, use the requests package\n",
    "import urllib.request\n",
    "filename = 'ex1.xlsx'\n",
    "url = 'https://github.com/wesm/pydata-book/blob/2nd-edition/examples/ex1.xlsx'\n",
    "urllib.request.urlretrieve(url, filename=filename)\n",
    "os.remove(filename)\n",
    "# CANNOT FIGURE OUT HOW TO DOWNLOAD THE EXCEL FILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with Web APIs\n",
    "request package can access web apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the last 30 GitHib issues on pandas on GitHub, we can make a GET HTTP request\n",
    "import requests\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resp.json() # returns a dictionary containing JSON parsed intonative Python objects\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['title'] # get the first index element [0] and then the 'title' key within the dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pass data directly to a dataframe\n",
    "issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state']) # creates a dataframe with the specified keys within the data that is a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS test\n",
    "(a VARCHAR(20), b VARCHAR(20),\n",
    " c REAL,        d INTEGER\n",
    ");\"\"\" # a,b,c,d, are the column names\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "con.execute(query)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data\n",
    "data = [('Atlanta', 'Georgia', 1.25, 6),\n",
    "        ('Tallahassee', 'Florida', 2.6, 3),\n",
    "        ('Sacramento', 'California', 1.7, 5)]\n",
    "stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
    "con.executemany(stmt, data) # execute more than one sql statements \n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples when selecting data from a table\n",
    "cursor = con.execute('select * from test') # gets a list of tuples\n",
    "rows = cursor.fetchall() # assigns tuples to rows\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.description # before we put this into a pandas dataframe, we need the colunm names\n",
    "pd.DataFrame(rows, columns =[x[0] for x in cursor.description]) # use a list comprehension to grapb the first index of the descriptons, which is the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SQLAlchemy roject makes it easier to work with sql databases\n",
    "import sqlalchemy as sqla\n",
    "db = sqla.create_engine('sqlite:///mydata.sqlite') # must use this exact string\n",
    "pd.read_sql('select * from test', db) # send query to database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc897af4fc6401c11359697fcbb52d9351578d8478d87d3dbab7c5724e632931"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
